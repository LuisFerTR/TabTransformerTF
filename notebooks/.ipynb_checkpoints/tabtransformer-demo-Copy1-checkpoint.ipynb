{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67febe95-dd8e-4564-8a5e-e641bb16906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tabtransformertf.models.tabtransformer import TabTransformer\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89b597-3f3a-4079-bc7c-ad3fbf16dba8",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c2547d-a0b7-478c-ae45-92e99dc6f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75521624-3f40-4aad-bbd4-cea5c5cb5782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race   gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country income_bracket  \n",
       "0          2174             0              40   United-States          <=50K  \n",
       "1             0             0              13   United-States          <=50K  \n",
       "2             0             0              40   United-States          <=50K  \n",
       "3             0             0              40   United-States          <=50K  \n",
       "4             0             0              40            Cuba          <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bff333-3750-46c9-b687-3f06a8f43845",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420201d9-eb7d-4873-bae8-2296fef046c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column information\n",
    "NUMERIC_FEATURES = train_data.select_dtypes(include=np.number).columns\n",
    "CATEGORICAL_FEATURES = train_data.select_dtypes(exclude=np.number).columns[:-1] # exclude label column and DT\n",
    "\n",
    "FEATURES = list(NUMERIC_FEATURES) + list(CATEGORICAL_FEATURES)\n",
    "LABEL = 'income_bracket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa2d685-44be-410a-89cf-acdd4a069adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2408095574460244, 0.23621176759611842)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding as binary target\n",
    "train_data[LABEL] = train_data[LABEL].apply(lambda x: int(x == ' >50K')) \n",
    "test_data[LABEL] = test_data[LABEL].apply(lambda x: int(x == ' >50K.'))\n",
    "train_data[LABEL].mean(), test_data[LABEL].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677298dc-05fe-47ef-b40a-f5b1d62e8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.iloc[1:, :] # drop invalid row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25986888-5dc1-497c-96c7-bc5e5411c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data types\n",
    "train_data[CATEGORICAL_FEATURES] = train_data[CATEGORICAL_FEATURES].astype(str)\n",
    "test_data[CATEGORICAL_FEATURES] = test_data[CATEGORICAL_FEATURES].astype(str)\n",
    "\n",
    "train_data[NUMERIC_FEATURES] = train_data[NUMERIC_FEATURES].astype(float)\n",
    "test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad185875-9f9e-43b2-9422-40fe9ff66d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_val = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b490426-51c8-4c9b-b530-df530c643787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_interp(x, xs, y) -> tf.Tensor:\n",
    "    # determine the output data type\n",
    "    ys = tf.convert_to_tensor(ys)\n",
    "    dtype = ys.dtype\n",
    "    \n",
    "    # normalize data types\n",
    "    ys = tf.cast(ys, tf.float64)\n",
    "    xs = tf.cast(xs, tf.float64)\n",
    "    x = tf.cast(x, tf.float64)\n",
    "\n",
    "    # pad control points for extrapolation\n",
    "    xs = tf.concat([[xs.dtype.min], xs, [xs.dtype.max]], axis=0)\n",
    "    ys = tf.concat([ys[:1], ys, ys[-1:]], axis=0)\n",
    "\n",
    "    # compute slopes, pad at the edges to flatten\n",
    "    ms = (ys[1:] - ys[:-1]) / (xs[1:] - xs[:-1])\n",
    "    ms = tf.pad(ms[:-1], [(1, 1)])\n",
    "\n",
    "    # solve for intercepts\n",
    "    bs = ys - ms*xs\n",
    "\n",
    "    # search for the line parameters at each input data point\n",
    "    # create a grid of the inputs and piece breakpoints for thresholding\n",
    "    # rely on argmax stopping on the first true when there are duplicates,\n",
    "    # which gives us an index into the parameter vectors\n",
    "    i = tf.math.argmax(xs[..., tf.newaxis, :] > x[..., tf.newaxis], axis=-1)\n",
    "    m = tf.gather(ms, i, axis=-1)\n",
    "    b = tf.gather(bs, i, axis=-1)\n",
    "\n",
    "    # apply the linear mapping at each input data point\n",
    "    y = m*x + b\n",
    "    return tf.cast(tf.reshape(y, tf.shape(x)), dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bad6b2fa-cf94-47cf-9bc2-a35eccae1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "class PLE(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_bins=10):\n",
    "        super(PLE, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    def adapt(self, data):\n",
    "        interval = 1/self.n_bins\n",
    "        bins = [np.quantile(data, q) for q in np.arange(0.0, 1 + interval, interval)]\n",
    "        init = tf.lookup.KeyValueTensorInitializer([i for i in range(len(bins))], bins)\n",
    "        self.lookup_table = tf.lookup.StaticHashTable(\n",
    "            init,\n",
    "            default_value=-1\n",
    "        )\n",
    "        self.lookup_size = self.lookup_table.size()\n",
    "    \n",
    "    def other_case(f, k, v):\n",
    "        e = (f - self.lookup_table.lookup(k-1)) / (v - self.lookup_table.lookup(k-1))\n",
    "        return e\n",
    "    \n",
    "    def ple_number(self, f):\n",
    "        ple = []\n",
    "        for i in tf.range(self.lookup_size):\n",
    "            if i != 0:\n",
    "                i = tf.cast(i, tf.int32)\n",
    "                v = self.lookup_table.lookup(i)\n",
    "                print(f)\n",
    "                zero = (f < self.lookup_table.lookup(i-1)) & (i > 1)\n",
    "                one = (f >= v) & (i < n_bins)\n",
    "                e = tf.cond(zero, lambda: 0., lambda: tf.cond(one, lambda: 1., lambda: other_case(f, i, v)))\n",
    "                ple.append(e)\n",
    "\n",
    "        ple = tf.stack(ple, axis=0)\n",
    "        return ple\n",
    "                                                              \n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.keras.layers.Flatten(x)\n",
    "        final_ple = []\n",
    "        for f in x:\n",
    "            final_ple.append(self.ple_number(f))\n",
    "\n",
    "        return tf.stack(final_ple)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "005d2084-d121-41d3-983c-dc093cad2349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 21., 42., 52., 39., 27., 66., 27., 22., 19., 72., 45., 48.,\n",
       "       30., 24., 73., 31., 44., 47., 23., 40., 38., 50., 20., 34., 45.,\n",
       "       42., 31., 33., 33., 17., 44., 37., 49., 21., 53., 25., 53., 17.,\n",
       "       45., 50., 53., 59., 23., 24., 29., 25., 24., 27., 42., 20., 72.,\n",
       "       40., 39., 21., 24., 40., 26., 22., 37., 25., 18., 31., 45., 23.,\n",
       "       44., 45., 28., 29., 19., 34., 51., 30., 57., 56., 42., 37., 26.,\n",
       "       39., 90., 45., 64., 57., 19., 30., 30., 42., 35., 39., 28., 37.,\n",
       "       57., 19., 40., 47., 32., 61., 22., 39., 22., 18., 27., 39., 54.,\n",
       "       29., 23., 19., 31., 21., 19., 37., 21., 25., 73., 18., 53., 35.,\n",
       "       46., 41., 19., 23., 61., 42., 39., 58., 90., 26., 51., 33., 46.,\n",
       "       20., 33., 32., 37., 44., 69., 36., 38., 31., 50., 37., 35., 40.,\n",
       "       30., 39., 42., 60., 18., 40., 44., 39., 28., 45., 18., 41., 52.,\n",
       "       45., 63., 25., 36., 69., 28., 37., 41., 42., 69., 51., 27., 65.,\n",
       "       35., 40., 39., 62., 38., 30., 30., 22., 45., 41., 49., 73., 25.,\n",
       "       48., 42., 32., 50., 68., 27., 26., 54., 57., 31., 52., 66., 47.,\n",
       "       34., 25., 37., 47., 46., 42., 46., 47., 55., 32., 31., 36., 24.,\n",
       "       36., 21., 17., 72., 70., 58., 66., 41., 49., 30., 30., 39., 56.,\n",
       "       41., 29., 34., 25., 59., 46., 33., 42., 21., 29., 19., 67., 43.,\n",
       "       22., 37., 23., 20., 33., 29., 56., 36., 37., 49., 31., 27., 48.,\n",
       "       49., 20., 38., 43., 36., 28., 23., 47., 46., 48., 40., 28., 35.,\n",
       "       18., 23., 39., 51., 32., 32., 57., 20., 34., 29., 19., 24., 26.,\n",
       "       52., 47., 62., 36., 47., 71., 53., 43., 30., 41., 54., 31., 18.,\n",
       "       49., 47., 31., 41., 31., 30., 19., 46., 52., 25., 31., 28., 32.,\n",
       "       46., 42., 26., 54., 27., 19., 50., 52., 40., 34., 24., 27., 45.,\n",
       "       46., 53., 34., 23., 35., 37., 35., 79., 24., 39., 32., 40., 40.,\n",
       "       60., 23., 28., 64., 27., 53., 57., 32., 29., 40., 28., 38., 24.,\n",
       "       54., 20., 65., 66., 48., 42., 32., 19., 49., 41., 26., 55., 26.,\n",
       "       56., 55., 27., 47., 39., 28., 59., 63., 23., 44., 58., 47., 18.,\n",
       "       32., 36., 21., 23., 54., 23., 25., 39., 36., 60., 25., 61., 20.,\n",
       "       55., 17., 41., 23., 48., 18., 42., 27., 46., 30., 74., 36., 17.,\n",
       "       61., 40., 42., 28., 29., 40., 61., 33., 40., 17., 54., 60., 61.,\n",
       "       36., 23., 26., 19., 42., 47., 23., 20., 78., 39., 64., 22., 28.,\n",
       "       47., 50., 51., 24., 27., 40., 43., 49., 36., 51., 49., 38., 36.,\n",
       "       31., 41., 47., 25., 39., 43., 51., 23., 33., 50., 28., 35., 48.,\n",
       "       52., 36., 46., 34., 38., 73., 37., 52., 51., 32., 27., 31., 38.,\n",
       "       59., 19., 32., 37., 32., 29., 37., 46., 37., 44., 31., 42., 18.,\n",
       "       43., 65., 19., 23., 46., 60., 40., 43., 22., 56., 21., 43., 48.,\n",
       "       24., 50., 33., 37., 46., 35., 39., 31., 29., 39., 19., 56., 22.,\n",
       "       47., 37., 40., 53., 37., 52., 44., 17., 35., 38., 26., 33., 64.,\n",
       "       46., 22., 29., 24., 22.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ba73221c-fdad-4401-be38-9f2470d4162b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"ple_31\" (type PLE).\n\ncannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:ConcatV2] name: concat\n\nCall arguments received:\n  • x=tf.Tensor(shape=(512, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m num_emb_layer\u001b[38;5;241m.\u001b[39madapt(X_train[NUMERIC_FEATURES[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     44\u001b[0m b \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mas_numpy_iterator()\u001b[38;5;241m.\u001b[39mnext()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mnum_emb_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m4\u001b[39m, :], b[:\u001b[38;5;241m4\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda/envs/blog/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36mPLE.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m right_mask \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mlookup(i)) \u001b[38;5;241m&\u001b[39m (i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_bins)\n\u001b[1;32m     29\u001b[0m v \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mlookup(i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mlookup(i) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mlookup(i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 30\u001b[0m left_masks \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m right_masks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([right_masks, right_mask], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m other_case \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([other_case, v], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"ple_31\" (type PLE).\n\ncannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:ConcatV2] name: concat\n\nCall arguments received:\n  • x=tf.Tensor(shape=(512, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "class PLE(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_bins=10):\n",
    "        super(PLE, self).__init__()\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "    def adapt(self, data):\n",
    "        interval = 1/self.n_bins\n",
    "        bins = [tf.cast(np.quantile(data, q), tf.float32) for q in np.arange(0.0, 1 + interval, interval)]\n",
    "        init = tf.lookup.KeyValueTensorInitializer([i for i in range(len(bins))], bins)\n",
    "        self.lookup_table = tf.lookup.StaticHashTable(\n",
    "            init,\n",
    "            default_value=-1\n",
    "        )\n",
    "        self.lookup_size = self.lookup_table.size()                                                 \n",
    "\n",
    "    def call(self, x):\n",
    "        ple_encoding_one = tf.ones((len(x), self.n_bins))\n",
    "        ple_encoding_zero = tf.zeros((len(x), self.n_bins))\n",
    "        \n",
    "        left_masks = tf.zeros([0])\n",
    "        right_masks = tf.zeros([0])\n",
    "        other_case = tf.zeros([0])\n",
    "        \n",
    "        for i in tf.range(1, self.n_bins+1):\n",
    "            left_mask = (x < self.lookup_table.lookup(i-1)) & (i > 1)\n",
    "            right_mask = (x >= self.lookup_table.lookup(i)) & (i < self.n_bins)\n",
    "            v = (x - self.lookup_table.lookup(i-1)) / (self.lookup_table.lookup(i) - self.lookup_table.lookup(i-1))\n",
    "            left_masks = tf.concat([left_masks, left_mask], axis=1)\n",
    "            right_masks = tf.concat([right_masks, right_mask], axis=1)\n",
    "            other_case = tf.concat([other_case, v], axis=1)\n",
    "        \n",
    "        other_mask = right_masks == left_masks # both are false\n",
    "        other_case = tf.cast(other_case, tf.float32)\n",
    "        enc = tf.where(left_masks, ple_encoding_zero, ple_encoding_one)\n",
    "        enc = tf.where(other_mask, other_case, enc)\n",
    "\n",
    "        return enc\n",
    "\n",
    "\n",
    "num_emb_layer = PLE(20)\n",
    "num_emb_layer.adapt(X_train[NUMERIC_FEATURES[0]].astype(np.float32))\n",
    "b = d.as_numpy_iterator().next()\n",
    "num_emb_layer(b)[:4, :], b[:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a53acc45-a8c3-477e-b004-b9f53a30b0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = tf.zeros([])\n",
    "tf.concat([my_list, [2, 2, 3]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d4756948-d19c-4ae8-ae46-7aa556a655cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = tf.TensorArray(tf.bool, size=0, dynamic_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a1d2dba-1c98-4e80-a807-8ba474398cb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorArray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m my_list \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorArray(tf\u001b[38;5;241m.\u001b[39mfloat32, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dynamic_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmy_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorArray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "my_list = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "my_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f363eea5-27dc-4f8a-b246-86a639a8be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy.random import randint\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# class PLE(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, bins):\n",
    "#         self.bins = bins\n",
    "        \n",
    "#     def fit(self, X):\n",
    "#         interval = 1 / self.bins\n",
    "#         self.bin_dict = {}\n",
    "#         for f in X.columns:\n",
    "#             bins = [np.quantile(X[f], q) for q in np.arange(0.0, 1.05, interval)]\n",
    "#             self.bin_dict[f] = {i: b for i, b in enumerate(bins)}\n",
    "        \n",
    "#         return self\n",
    "    \n",
    "#     def ple_embed(self, f, bin_dict):\n",
    "#         ple = []\n",
    "#         n_bins = len(bin_dict.items())\n",
    "#         for k, v in islice(bin_dict.items(), 1, None):    \n",
    "#             if (f < bin_dict[k-1]) & (k > 1):\n",
    "#                 ple.append(0.)\n",
    "#             elif (f >= v) & (k < n_bins):\n",
    "#                 ple.append(1.)\n",
    "#             else:\n",
    "#                 e = np.round((f - bin_dict[k-1]) / (v - bin_dict[k-1]), 5)\n",
    "#                 ple.append(e)\n",
    "\n",
    "#         return ple\n",
    "    \n",
    "#     def transform(self, X):\n",
    "#         for f in X.columns:\n",
    "#             X[f] = X[f].apply(lambda x: self.ple_embed(x, self.bin_dict[f]))\n",
    "\n",
    "#         return X\n",
    "    \n",
    "# ple_transfromer = PLE(bins=20)\n",
    "# ple_transfromer.fit(X_train[NUMERIC_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f38fd13-934f-4a4d-a530-a0982c853851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[NUMERIC_FEATURES] = ple_transfromer.transform(X_train[NUMERIC_FEATURES])\n",
    "# X_val[NUMERIC_FEATURES] = ple_transfromer.transform(X_val[NUMERIC_FEATURES])\n",
    "# test_data[NUMERIC_FEATURES] = ple_transfromer.transform(test_data[NUMERIC_FEATURES])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ad92f-f1b9-46ad-9b61-831cbc903e22",
   "metadata": {},
   "source": [
    "## Modelling Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1592a15a-cf05-4a35-ba46-69e877998d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 235.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Category preprocessing layers\n",
    "category_prep_layers = build_categorical_prep(X_train, CATEGORICAL_FEATURES)\n",
    "\n",
    "numerical_prep_layers = {}\n",
    "# Numerical prep layers\n",
    "for f in NUMERIC_FEATURES:\n",
    "    num_emb_layer = PLE(20)\n",
    "    num_emb_layer.adapt(X_train[f])\n",
    "    numerical_prep_layers[f] = num_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9bfdb641-4fec-4f2e-a84f-149a41e2a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To TF Dataset\n",
    "X_train[NUMERIC_FEATURES] = X_train[NUMERIC_FEATURES].astype(np.float32)\n",
    "X_val[NUMERIC_FEATURES] = X_val[NUMERIC_FEATURES].astype(np.float32)\n",
    "test_data[NUMERIC_FEATURES] = test_data[NUMERIC_FEATURES].astype(np.float32)\n",
    "\n",
    "train_dataset = df_to_dataset(X_train[FEATURES + [LABEL]], LABEL)\n",
    "val_dataset = df_to_dataset(X_val[FEATURES + [LABEL]], LABEL, shuffle=False)  # No shuffle\n",
    "test_dataset = df_to_dataset(test_data[FEATURES + [LABEL]], shuffle=False) # No target, no shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe05de6-639b-48b6-8387-e44be0d8db0c",
   "metadata": {},
   "source": [
    "## TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b169bd3e-d41a-44f7-ae82-7d1e49bdb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "from tabtransformertf.utils.helper import build_mlp\n",
    "from tabtransformertf.models.tabtransformer import TransformerBlock\n",
    "\n",
    "\n",
    "\n",
    "class TabTransformerEncoder(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features: list,\n",
    "        numerical_features: list,\n",
    "        categorical_lookup: dict,\n",
    "        embedding_dim: int = 32,\n",
    "        depth: int = 4,\n",
    "        heads: int = 8,\n",
    "        attn_dropout: float = 0.1,\n",
    "        ff_dropout: float = 0.1,\n",
    "        numerical_embeddings: dict = None,\n",
    "        use_column_embedding: bool = True,\n",
    "    ):\n",
    "        \"\"\"TabTransformer Tensorflow Model\n",
    "        Args:\n",
    "            categorical_features (list): names of categorical features\n",
    "            numerical_features (list): names of numeric features\n",
    "            categorical_lookup (dict): dictionary with categorical feature names as keys and adapted StringLookup layers as values\n",
    "            out_dim (int): model output dimensions\n",
    "            out_activation (str): model output activation\n",
    "            embedding_dim (int, optional): embedding dimensions. Defaults to 32.\n",
    "            depth (int, optional): number of transformer blocks. Defaults to 4.\n",
    "            heads (int, optional): number of attention heads. Defaults to 8.\n",
    "            attn_dropout (float, optional): dropout rate in transformer. Defaults to 0.1.\n",
    "            ff_dropout (float, optional): dropout rate in mlps. Defaults to 0.1.\n",
    "            mlp_hidden_factors (list[int], optional): numbers by which we divide dimensionality. Defaults to [2, 4].\n",
    "            numerical_discretisers (dict, optional): dictionary with numerical feature names as keys and adapted Discretizer and IntegerLookup layers as values. Defaults to None.\n",
    "            use_column_embedding (bool, optional): flag to use fixed column positional embeddings. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        super(TabTransformerEncoder, self).__init__()\n",
    "        self.numerical = numerical_features\n",
    "        self.categorical = categorical_features\n",
    "        self.embed_numeric = numerical_embeddings is not None\n",
    "        self.num_categories = [\n",
    "            categorical_lookup[c].vocabulary_size() for c in self.categorical\n",
    "        ]\n",
    "\n",
    "        # ---------- Numerical Input -----------\n",
    "        if len(self.numerical) > 0:\n",
    "            # If we want to quantise numeric features\n",
    "            if self.embed_numeric:\n",
    "                # Layers to transform numeric into embedding\n",
    "                self.numerical_embeddings = numerical_embeddings\n",
    "                # Linear layer after embedding\n",
    "                self.numerical_embedding_linear = [\n",
    "                    Dense(embedding_dim, activation='relu') for n in self.numerical\n",
    "                ]\n",
    "                \n",
    "            else:\n",
    "                # If not quantising, then simply normalise and concatenate\n",
    "                self.continuous_normalization = LayerNormalization()\n",
    "                self.numerical_concatenation = Concatenate(axis=1)\n",
    "\n",
    "        # ---------- Categorical Input -----------\n",
    "\n",
    "        # String lookups for categorical\n",
    "        self.categorical_lookups = [categorical_lookup[c] for c in self.categorical]\n",
    "\n",
    "        # Categorical input embedding\n",
    "        self.cat_embedding_layers = []\n",
    "        for number_of_classes in self.num_categories:\n",
    "            category_embedding = Embedding(\n",
    "                input_dim=number_of_classes, output_dim=embedding_dim\n",
    "            )\n",
    "            self.cat_embedding_layers.append(category_embedding)\n",
    "\n",
    "        # Column embedding\n",
    "        self.use_column_embedding = use_column_embedding\n",
    "        if use_column_embedding:\n",
    "            num_columns = len(self.categorical)\n",
    "            if self.embed_numeric:\n",
    "                num_columns += len(self.numerical)\n",
    "            self.column_embedding = Embedding(\n",
    "                input_dim=num_columns, output_dim=embedding_dim\n",
    "            )\n",
    "            self.column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "\n",
    "        # Embedding concatenation layer\n",
    "        self.embedded_concatenation = Concatenate(axis=1)\n",
    "\n",
    "        # adding transformers\n",
    "        self.transformers = []\n",
    "        for _ in range(depth):\n",
    "            self.transformers.append(\n",
    "                TransformerBlock(\n",
    "                    embedding_dim,\n",
    "                    heads,\n",
    "                    embedding_dim,\n",
    "                    att_dropout=attn_dropout,\n",
    "                    ff_dropout=ff_dropout,\n",
    "                )\n",
    "            )\n",
    "        self.flatten_transformer_output = Flatten()\n",
    "\n",
    "        # MLP\n",
    "        self.pre_mlp_concatenation = Concatenate()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        numerical_feature_list = []\n",
    "        categorical_feature_list = []\n",
    "        print(self.numerical_embeddings)\n",
    "\n",
    "        if len(self.numerical) > 0:\n",
    "            # Each numeric feature needs to be binned, looked up, and embedded\n",
    "            for i, n in enumerate(self.numerical):\n",
    "                if self.embed_numeric:\n",
    "                    print(self.numerical_embeddings[n])\n",
    "                    num_embedded = self.numerical_embeddings[n](inputs[n])\n",
    "                    print(num_embedded)\n",
    "                    num_embedded = self.numerical_embedding_linear[i](num_embedded)\n",
    "                    numerical_feature_list.append(num_embedded)\n",
    "                else:\n",
    "                    # Otherwise we pass it as it is\n",
    "                    numerical_feature_list.append(inputs[n])\n",
    "        \n",
    "        print('passed numeric')\n",
    "\n",
    "        for i, c in enumerate(self.categorical):\n",
    "            cat_encoded = self.categorical_lookups[i](inputs[c])\n",
    "            cat_embedded = self.cat_embedding_layers[i](cat_encoded)\n",
    "            categorical_feature_list.append(cat_embedded)\n",
    "\n",
    "        if self.embed_numeric:\n",
    "            # Stack categorical embeddings for the Tansformer.\n",
    "            transformer_inputs = self.embedded_concatenation(\n",
    "                numerical_feature_list + categorical_feature_list\n",
    "            )\n",
    "        else:\n",
    "            transformer_inputs = self.embedded_concatenation(categorical_feature_list)\n",
    "\n",
    "        if self.use_column_embedding:\n",
    "            # Add column embeddings\n",
    "            transformer_inputs += self.column_embedding(self.column_indices)\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            transformer_inputs = transformer(transformer_inputs)\n",
    "\n",
    "        # Flatten the \"contextualized\" embeddings of the features.\n",
    "        mlp_input = self.flatten_transformer_output(transformer_inputs)\n",
    "\n",
    "        # In case we don't quantize, we want to normalise and concatenate numerical features with embeddings\n",
    "        if (self.embed_numeric is False) and (len(self.numerical) > 0):\n",
    "            numerical_inputs = self.numerical_concatenation(numerical_feature_list)\n",
    "            numerical_inputs = self.continuous_normalization(numerical_inputs)\n",
    "            mlp_input = self.pre_mlp_concatenation([mlp_input, numerical_inputs])\n",
    "\n",
    "        return mlp_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e29156d1-7d28-439b-9e00-2a98e38f1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': <__main__.PLE object at 0x7fd890841310>, 'fnlwgt': <__main__.PLE object at 0x7fd88ff7c850>, 'education_num': <__main__.PLE object at 0x7fd8912a2700>, 'capital_gain': <__main__.PLE object at 0x7fd8900261f0>, 'capital_loss': <__main__.PLE object at 0x7fd890834880>, 'hours_per_week': <__main__.PLE object at 0x7fd890834b80>}\n",
      "<__main__.PLE object at 0x7fd890841310>\n",
      "[<tf.Tensor 'tab_transformer_encoder_1/ple_25/while/and:0' shape=(None, 1) dtype=bool>]\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    InaccessibleTensorError: Exception encountered when calling layer \"tab_transformer_encoder_1\" (type TabTransformerEncoder).\n    \n    in user code:\n    \n        File \"/var/folders/66/1klxbkpn5vdgpvqwt_hmtn5c0000gn/T/ipykernel_39366/3156529361.py\", line 129, in call  *\n            num_embedded = self.numerical_embeddings[n](inputs[n])\n        File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        InaccessibleTensorError: Exception encountered when calling layer \"ple_25\" (type PLE).\n        \n        in user code:\n        \n            File \"/var/folders/66/1klxbkpn5vdgpvqwt_hmtn5c0000gn/T/ipykernel_39366/528994165.py\", line 35, in call  *\n                left_masks = tf.concat(left_masks, axis=1)\n        \n            InaccessibleTensorError: tf.Graph captured an external symbolic tensor. The symbolic tensor <tf.Tensor 'tab_transformer_encoder_1/ple_25/while/and:0' shape=(None, 1) dtype=bool> is captured by FuncGraph(name=predict_function, id=140568045454720), but it is defined at FuncGraph(name=tab_transformer_encoder_1_ple_25_while_body_17721, id=140568118298656). A tf.Graph is not allowed to capture symoblic tensors from another graph. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n            \n        \n        \n        Call arguments received:\n          • x=tf.Tensor(shape=(None, 1), dtype=float32)\n    \n    \n    Call arguments received:\n      • inputs={'age': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'fnlwgt': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'education_num': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'capital_gain': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'capital_loss': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'hours_per_week': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'workclass': 'tf.Tensor(shape=(None, 1), dtype=string)', 'education': 'tf.Tensor(shape=(None, 1), dtype=string)', 'marital_status': 'tf.Tensor(shape=(None, 1), dtype=string)', 'occupation': 'tf.Tensor(shape=(None, 1), dtype=string)', 'relationship': 'tf.Tensor(shape=(None, 1), dtype=string)', 'race': 'tf.Tensor(shape=(None, 1), dtype=string)', 'gender': 'tf.Tensor(shape=(None, 1), dtype=string)', 'native_country': 'tf.Tensor(shape=(None, 1), dtype=string)'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m tab \u001b[38;5;241m=\u001b[39m TabTransformerEncoder(\n\u001b[1;32m      2\u001b[0m     numerical_features \u001b[38;5;241m=\u001b[39m NUMERIC_FEATURES,\n\u001b[1;32m      3\u001b[0m     categorical_features \u001b[38;5;241m=\u001b[39m CATEGORICAL_FEATURES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mtab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/blog/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/envs/blog/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/engine/training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    InaccessibleTensorError: Exception encountered when calling layer \"tab_transformer_encoder_1\" (type TabTransformerEncoder).\n    \n    in user code:\n    \n        File \"/var/folders/66/1klxbkpn5vdgpvqwt_hmtn5c0000gn/T/ipykernel_39366/3156529361.py\", line 129, in call  *\n            num_embedded = self.numerical_embeddings[n](inputs[n])\n        File \"/Users/antonsruberts/miniconda/envs/blog/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        InaccessibleTensorError: Exception encountered when calling layer \"ple_25\" (type PLE).\n        \n        in user code:\n        \n            File \"/var/folders/66/1klxbkpn5vdgpvqwt_hmtn5c0000gn/T/ipykernel_39366/528994165.py\", line 35, in call  *\n                left_masks = tf.concat(left_masks, axis=1)\n        \n            InaccessibleTensorError: tf.Graph captured an external symbolic tensor. The symbolic tensor <tf.Tensor 'tab_transformer_encoder_1/ple_25/while/and:0' shape=(None, 1) dtype=bool> is captured by FuncGraph(name=predict_function, id=140568045454720), but it is defined at FuncGraph(name=tab_transformer_encoder_1_ple_25_while_body_17721, id=140568118298656). A tf.Graph is not allowed to capture symoblic tensors from another graph. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n            \n        \n        \n        Call arguments received:\n          • x=tf.Tensor(shape=(None, 1), dtype=float32)\n    \n    \n    Call arguments received:\n      • inputs={'age': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'fnlwgt': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'education_num': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'capital_gain': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'capital_loss': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'hours_per_week': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'workclass': 'tf.Tensor(shape=(None, 1), dtype=string)', 'education': 'tf.Tensor(shape=(None, 1), dtype=string)', 'marital_status': 'tf.Tensor(shape=(None, 1), dtype=string)', 'occupation': 'tf.Tensor(shape=(None, 1), dtype=string)', 'relationship': 'tf.Tensor(shape=(None, 1), dtype=string)', 'race': 'tf.Tensor(shape=(None, 1), dtype=string)', 'gender': 'tf.Tensor(shape=(None, 1), dtype=string)', 'native_country': 'tf.Tensor(shape=(None, 1), dtype=string)'}\n"
     ]
    }
   ],
   "source": [
    "tab = TabTransformerEncoder(\n",
    "    numerical_features = NUMERIC_FEATURES,\n",
    "    categorical_features = CATEGORICAL_FEATURES,\n",
    "    categorical_lookup=category_prep_layers,\n",
    "    numerical_embeddings=numerical_prep_layers,\n",
    "    embedding_dim=16\n",
    ")\n",
    "preds = tab.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67528470-3cf1-4c6a-8b62-c701f7eb774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabtransformer = TabTransformer(\n",
    "    numerical_features = NUMERIC_FEATURES,\n",
    "    categorical_features = CATEGORICAL_FEATURES,\n",
    "    categorical_lookup=category_prep_layers,\n",
    "    numerical_embeddings=\n",
    "    numerical_discretisers=None, # simply passing the numeric features\n",
    "    embedding_dim=32,\n",
    "    out_dim=1,\n",
    "    out_activation='sigmoid',\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    attn_dropout=0.2,\n",
    "    ff_dropout=0.2,\n",
    "    mlp_hidden_factors=[2, 4],\n",
    "    use_column_embedding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9a2389-2a47-422b-a365-002fb5c6fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "tabtransformer.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics= [tf.keras.metrics.AUC(name=\"PR AUC\", curve='PR')],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d6dfba2-3d79-45a2-ba1c-b954eda73432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "51/51 [==============================] - 13s 166ms/step - loss: 0.6778 - PR AUC: 0.4680 - val_loss: 0.5222 - val_PR AUC: 0.6302\n",
      "Epoch 2/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.5601 - PR AUC: 0.5694 - val_loss: 0.4100 - val_PR AUC: 0.6429\n",
      "Epoch 3/1000\n",
      "51/51 [==============================] - 8s 151ms/step - loss: 0.4897 - PR AUC: 0.6019 - val_loss: 0.3768 - val_PR AUC: 0.6466\n",
      "Epoch 4/1000\n",
      "51/51 [==============================] - 8s 150ms/step - loss: 0.4482 - PR AUC: 0.6226 - val_loss: 0.3749 - val_PR AUC: 0.6581\n",
      "Epoch 5/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.4239 - PR AUC: 0.6342 - val_loss: 0.3682 - val_PR AUC: 0.6722\n",
      "Epoch 6/1000\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.4096 - PR AUC: 0.6547 - val_loss: 0.3626 - val_PR AUC: 0.6798\n",
      "Epoch 7/1000\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.4045 - PR AUC: 0.6589 - val_loss: 0.3620 - val_PR AUC: 0.6863\n",
      "Epoch 8/1000\n",
      "51/51 [==============================] - 9s 164ms/step - loss: 0.3932 - PR AUC: 0.6752 - val_loss: 0.3572 - val_PR AUC: 0.6924\n",
      "Epoch 9/1000\n",
      "51/51 [==============================] - 9s 164ms/step - loss: 0.3921 - PR AUC: 0.6708 - val_loss: 0.3512 - val_PR AUC: 0.6994\n",
      "Epoch 10/1000\n",
      "51/51 [==============================] - 8s 158ms/step - loss: 0.3919 - PR AUC: 0.6720 - val_loss: 0.3477 - val_PR AUC: 0.7050\n",
      "Epoch 11/1000\n",
      "51/51 [==============================] - 8s 158ms/step - loss: 0.3852 - PR AUC: 0.6755 - val_loss: 0.3506 - val_PR AUC: 0.7068\n",
      "Epoch 12/1000\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.3819 - PR AUC: 0.6833 - val_loss: 0.3451 - val_PR AUC: 0.7108\n",
      "Epoch 13/1000\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.3802 - PR AUC: 0.6873 - val_loss: 0.3438 - val_PR AUC: 0.7120\n",
      "Epoch 14/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.3780 - PR AUC: 0.6914 - val_loss: 0.3426 - val_PR AUC: 0.7144\n",
      "Epoch 15/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.3765 - PR AUC: 0.6897 - val_loss: 0.3418 - val_PR AUC: 0.7165\n",
      "Epoch 16/1000\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.3706 - PR AUC: 0.6977 - val_loss: 0.3424 - val_PR AUC: 0.7147\n",
      "Epoch 17/1000\n",
      "51/51 [==============================] - 8s 156ms/step - loss: 0.3718 - PR AUC: 0.6951 - val_loss: 0.3416 - val_PR AUC: 0.7186\n",
      "Epoch 18/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3702 - PR AUC: 0.6958 - val_loss: 0.3399 - val_PR AUC: 0.7196\n",
      "Epoch 19/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3672 - PR AUC: 0.7016 - val_loss: 0.3406 - val_PR AUC: 0.7192\n",
      "Epoch 20/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3652 - PR AUC: 0.7027 - val_loss: 0.3384 - val_PR AUC: 0.7209\n",
      "Epoch 21/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.3644 - PR AUC: 0.7052 - val_loss: 0.3376 - val_PR AUC: 0.7211\n",
      "Epoch 22/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3642 - PR AUC: 0.7034 - val_loss: 0.3385 - val_PR AUC: 0.7205\n",
      "Epoch 23/1000\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.3617 - PR AUC: 0.7094 - val_loss: 0.3372 - val_PR AUC: 0.7225\n",
      "Epoch 24/1000\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.3598 - PR AUC: 0.7116 - val_loss: 0.3385 - val_PR AUC: 0.7223\n",
      "Epoch 25/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.3585 - PR AUC: 0.7129 - val_loss: 0.3406 - val_PR AUC: 0.7214\n",
      "Epoch 26/1000\n",
      "51/51 [==============================] - 8s 157ms/step - loss: 0.3597 - PR AUC: 0.7112 - val_loss: 0.3370 - val_PR AUC: 0.7226\n",
      "Epoch 27/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3575 - PR AUC: 0.7137 - val_loss: 0.3368 - val_PR AUC: 0.7232\n",
      "Epoch 28/1000\n",
      "51/51 [==============================] - 8s 159ms/step - loss: 0.3569 - PR AUC: 0.7139 - val_loss: 0.3367 - val_PR AUC: 0.7230\n",
      "Epoch 29/1000\n",
      "51/51 [==============================] - 10s 194ms/step - loss: 0.3535 - PR AUC: 0.7174 - val_loss: 0.3365 - val_PR AUC: 0.7238\n",
      "Epoch 30/1000\n",
      "51/51 [==============================] - 11s 216ms/step - loss: 0.3532 - PR AUC: 0.7208 - val_loss: 0.3373 - val_PR AUC: 0.7231\n",
      "Epoch 31/1000\n",
      "51/51 [==============================] - 9s 179ms/step - loss: 0.3526 - PR AUC: 0.7186 - val_loss: 0.3371 - val_PR AUC: 0.7243\n",
      "Epoch 32/1000\n",
      "51/51 [==============================] - 9s 178ms/step - loss: 0.3534 - PR AUC: 0.7173 - val_loss: 0.3367 - val_PR AUC: 0.7243\n",
      "Epoch 33/1000\n",
      "51/51 [==============================] - 8s 162ms/step - loss: 0.3513 - PR AUC: 0.7235 - val_loss: 0.3369 - val_PR AUC: 0.7236\n",
      "Epoch 34/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.3502 - PR AUC: 0.7238 - val_loss: 0.3375 - val_PR AUC: 0.7226\n",
      "Epoch 35/1000\n",
      "51/51 [==============================] - 8s 150ms/step - loss: 0.3476 - PR AUC: 0.7277 - val_loss: 0.3391 - val_PR AUC: 0.7225\n",
      "Epoch 36/1000\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.3491 - PR AUC: 0.7259 - val_loss: 0.3388 - val_PR AUC: 0.7229\n",
      "Epoch 37/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3497 - PR AUC: 0.7251 - val_loss: 0.3375 - val_PR AUC: 0.7235\n",
      "Epoch 38/1000\n",
      "51/51 [==============================] - 8s 154ms/step - loss: 0.3493 - PR AUC: 0.7256 - val_loss: 0.3371 - val_PR AUC: 0.7240\n",
      "Epoch 39/1000\n",
      "51/51 [==============================] - 8s 160ms/step - loss: 0.3473 - PR AUC: 0.7260 - val_loss: 0.3359 - val_PR AUC: 0.7238\n",
      "Epoch 40/1000\n",
      "51/51 [==============================] - 8s 158ms/step - loss: 0.3483 - PR AUC: 0.7256 - val_loss: 0.3353 - val_PR AUC: 0.7245\n",
      "Epoch 41/1000\n",
      "51/51 [==============================] - 9s 165ms/step - loss: 0.3487 - PR AUC: 0.7249 - val_loss: 0.3371 - val_PR AUC: 0.7243\n",
      "Epoch 42/1000\n",
      "51/51 [==============================] - 9s 166ms/step - loss: 0.3455 - PR AUC: 0.7290 - val_loss: 0.3354 - val_PR AUC: 0.7243\n",
      "Epoch 43/1000\n",
      "51/51 [==============================] - 8s 163ms/step - loss: 0.3473 - PR AUC: 0.7247 - val_loss: 0.3360 - val_PR AUC: 0.7242\n",
      "Epoch 44/1000\n",
      "51/51 [==============================] - 8s 162ms/step - loss: 0.3451 - PR AUC: 0.7289 - val_loss: 0.3363 - val_PR AUC: 0.7247\n",
      "Epoch 45/1000\n",
      "51/51 [==============================] - 9s 166ms/step - loss: 0.3458 - PR AUC: 0.7269 - val_loss: 0.3363 - val_PR AUC: 0.7247\n",
      "Epoch 46/1000\n",
      "51/51 [==============================] - 9s 164ms/step - loss: 0.3455 - PR AUC: 0.7296 - val_loss: 0.3363 - val_PR AUC: 0.7238\n",
      "Epoch 47/1000\n",
      "51/51 [==============================] - 9s 166ms/step - loss: 0.3439 - PR AUC: 0.7315 - val_loss: 0.3359 - val_PR AUC: 0.7238\n",
      "Epoch 48/1000\n",
      "51/51 [==============================] - 9s 167ms/step - loss: 0.3416 - PR AUC: 0.7328 - val_loss: 0.3356 - val_PR AUC: 0.7242\n",
      "Epoch 49/1000\n",
      "51/51 [==============================] - 9s 169ms/step - loss: 0.3439 - PR AUC: 0.7313 - val_loss: 0.3360 - val_PR AUC: 0.7246\n",
      "Epoch 50/1000\n",
      "51/51 [==============================] - 9s 171ms/step - loss: 0.3442 - PR AUC: 0.7300 - val_loss: 0.3383 - val_PR AUC: 0.7245\n"
     ]
    }
   ],
   "source": [
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, restore_best_weights=True)\n",
    "callback_list = [early]\n",
    "\n",
    "history = tabtransformer.fit(\n",
    "    train_dataset, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callback_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f62dea2-531c-4e8b-991c-37c1f5e73fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = tabtransformer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2852b3bf-73d4-46c5-9daf-289d9190168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.8956\n",
      "Test PR AUC: 0.7343\n"
     ]
    }
   ],
   "source": [
    "print(\"Test ROC AUC:\", np.round(roc_auc_score(test_data[LABEL], test_preds.ravel()), 4))\n",
    "print(\"Test PR AUC:\", np.round(average_precision_score(test_data[LABEL], test_preds.ravel()), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
